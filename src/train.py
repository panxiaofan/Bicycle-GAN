import warnings
warnings.filterwarnings("ignore")
from torch.utils import data
from torch import nn, optim
from vis_tools import *
from datasets import *
from models import *
import argparse, os
import itertools
import torch
import time
import pdb
from torch.autograd import Variable

# Training Configurations 
# (You may put your needed configuration here. Please feel free to add more or use argparse. )
# img_dir = '/home/zlz/BicycleGAN/datasets/edges2shoes/train/'
img_dir = '../data/train/'
img_shape = (3, 128, 128) # Please use this image dimension faster training purpose
num_epochs = 2
batch_size = 2
lr_rate = 0.0002  	      # Adam optimizer learning rate
beta1 = 0.5			  # Adam optimizer beta 1, beta 2
beta2 = 0.999
lambda_pixel =  10      # Loss weights for pixel loss
lambda_latent =  0.5     # Loss weights for latent regression
lambda_kl =  0.01         # Loss weights for kl divergence
latent_dim =  8        # latent dimension for the encoded images from domain B

# Normalize image tensor
def norm(image):
    return (image/255.0-0.5)*2.0

# Denormalize image tensor
def denorm(tensor):
    return ((tensor+1.0)/2.0)*255.0

# Reparameterization helper function 
# (You may need this helper function here or inside models.py, depending on your encoder implementation)

def reparameterization(mu, logvar):    
    std = torch.exp(logvar/2)
    eps = torch.randn_like(std)
    z = mu + std*eps
    return z


##############################
#        Loss
##############################

# calculate the L-1 loss between transformed image G(A, z) and target image B
def loss_generator(G, real_img, z, target_img, criterion_l1):
    '''
    Input:
    G - generator
    real_img - the original image, A
    z - encoded z, the output of raparameterization
    target_img - the target image, B
    
    Output:
    L1 loss between image generated by generator and our target image B
    '''
    # generating image
    fake_img = G(real_img, z)
    assert fake_img.shape == target_img.shape
    return criterion_l1(fake_img, target_img)

# calculate the adversarial loss
def loss_discriminator(fake_img, D, real_img, criterion_bce, Tensor):
    '''
    This function could be used for the adversarial loss in both cVAE-GAN and cLR-GAN
    Input:
    fake_img - fake images generate by the generator. 
    D - Discriminator
    real_img: original image A
    valid: label of valid output, which equals to 1
    fake: label of invalid output, which equals to 0
    
    Output:
    - loss_D: adversarial loss
    '''
    # since we use PatchGan Discriminator, we return two outputs
    real_d_out_1,  real_d_out_2 = D(real_img)
    valid_1 = Variable(Tensor(np.ones(real_d_out_1.shape)), requires_grad=False)
    valid_2 = Variable(Tensor(np.ones(real_d_out_2.shape)), requires_grad=False)
    real_loss_1 = criterion_bce(real_d_out_1, valid_1)
    real_loss_2 = criterion_bce(real_d_out_2, valid_2)

    fake_d_out_1, fake_d_out_2 = D(fake_img)
    fake_1 = Variable(Tensor(np.zeros(fake_d_out_1.shape)), requires_grad=False)
    fake_2 = Variable(Tensor(np.zeros(fake_d_out_2.shape)), requires_grad=False)

    fake_loss_1 = criterion_bce(fake_d_out_1, fake_1)
    fake_loss_2 = criterion_bce(fake_d_out_2, fake_2)

    # loss_D = (real_loss_1 + real_loss_2)/2 + (fake_loss_1 + fake_loss_2)/2
    loss_D = real_loss_1 + fake_loss_1 + real_loss_2 +  fake_loss_2

    return loss_D

def loss_discriminator_fix_D(fake_img, D, criterion_mse, Tensor):
    fake_d_1, fake_d_2 = D(fake_img.detach())
    valid1 = Variable(Tensor(np.ones(fake_d_1.shape)), requires_grad=False)
    valid2 = Variable(Tensor(np.ones(fake_d_2.shape)), requires_grad=False)
    fake_loss_1 = criterion_mse(fake_d_1, valid1)
    fake_loss_2 = criterion_mse(fake_d_2, valid2)
    loss_vae_gan = fake_loss_1 + fake_loss_2
    return loss_vae_gan




# compute the KL-Divergence between N(0,1) and latent space
def kld(mu, logvar):
    '''
    Input:
    mu - output of the Encoder, the mean of normal distribution
    logvar - output of the Encoder, the log value of variance
    
    Output:
    KL-divengence
    '''
    
    return torch.sum(0.5*(mu ** 2 + torch.exp(logvar) - logvar - 1))
    

def z_loss(fake_img, encoder, random_z, criterion_l1):
    '''
    Input:
    fake_img - fake images generated by the generator, B_hat
    encoder - the trained Encoder
    random_z - randomly generated standard normal distribution, the prior distribution of p(z)
    
    Output:
    L1 los between the encoded latent vector of B_hat and prior distribution p(z)
    '''
    mu, logvar = encoder(fake_img)
    z_loss = criterion_l1(mu, random_z)
    return z_loss

def all_zero_grad(optimizer1, optimizer2, optimizer3, optimizer4):
    optimizer1.zero_grad()
    optimizer2.zero_grad()
    optimizer3.zero_grad()
    optimizer4.zero_grad()
   

# Random seeds (optional)
torch.manual_seed(1); np.random.seed(1)
# Define DataLoader
dataset = Edge2Shoe(img_dir)
loader = data.DataLoader(dataset, batch_size=batch_size)
gpu_id = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

# Loss functions
# criterion_l1 = torch.nn.L1Loss().to(gpu_id)
# criterion_mse = torch.nn.MSELoss().to(gpu_id)
criterion_l1 = torch.nn.L1Loss().cuda()
criterion_mse = torch.nn.MSELoss().cuda()

# Define generator, encoder and discriminators
encoder = Encoder(latent_dim).cuda()
generator = Generator(latent_dim, img_shape).cuda()
D_VAE = Discriminator().cuda()
D_LR = Discriminator().cuda()

# Define optimizers for networks
optimizer_E = torch.optim.Adam(encoder.parameters(), lr=lr_rate, betas=(beta1,beta2))
optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_rate, betas=(beta1,beta2))
optimizer_D_VAE = torch.optim.Adam(D_VAE.parameters(), lr=lr_rate, betas=(beta1,beta2))
optimizer_D_LR = torch.optim.Adam(D_LR.parameters(), lr=lr_rate, betas=(beta1,beta2))

# For adversarial loss (optional to use)
Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor
torch.autograd.set_detect_anomaly(True)
# losss recorder
running_loss_l1_image = 0
running_loss_gan_vae = 0
running_loss_kl = 0
running_loss_l1_z = 0
running_loss_gan = 0

loss_l1_image_list = []
loss_gan_vae_list = []
loss_kl_list = []
loss_l1_z_list = []
loss_gan_list = []
total_loss_list = []
# Training
total_steps = len(loader)*num_epochs; step = 0

generator.train()
encoder.train()
D_LR.train()
D_VAE.train()

for e in range(num_epochs):
    start = time.time()
    for idx, data in enumerate(loader):

        ########## Process Inputs ##########
        edge_tensor, rgb_tensor = data
        # edge_tensor, rgb_tensor = norm(edge_tensor).to(gpu_id), norm(rgb_tensor).to(gpu_id
        edge_tensor, rgb_tensor = norm(edge_tensor).cuda(), norm(rgb_tensor).cuda()
        real_A = edge_tensor
        real_B = rgb_tensor
        #----------------------------------
        #  Train Discriminator (cVAE-GAN)
        #----------------------------------
        loss_D = 0
        # generate loss in cVAE-GAN
        mu, logvar = encoder(real_B)
        z = reparameterization(mu, logvar)
        fake_B_cVAE = generator(real_A, z)
        loss_d_gan_vae = loss_discriminator(fake_B_cVAE, D_VAE, real_B, criterion_mse, Tensor)
        # generate loss in cLR-GAN
        random_z = Variable(Tensor(np.random.normal(0, 1, (batch_size,latent_dim))), requires_grad=False)
        fake_B_cLR = generator(real_A, random_z)
        loss_d_gan = loss_discriminator(fake_B_cLR, D_LR, real_B, criterion_mse, Tensor)
        loss_D = loss_d_gan_vae + loss_d_gan
        #update
        all_zero_grad(optimizer_G, optimizer_E, optimizer_D_VAE, optimizer_D_LR)
        loss_D.backward()
        optimizer_D_VAE.step()
        optimizer_D_LR.step()
        #-------------------------------
        #  Train Generator and Encoder
        #------------------------------
        loss_EG = 0
        loss_G = 0
        # GAN loss in cVAE-GAN
        mu, logvar = encoder(real_B)
        z = reparameterization(mu, logvar)
        fake_B_cVAE = generator(real_A, z)
        loss_G_cVAE = loss_discriminator_fix_D(fake_B_cVAE, D_VAE, criterion_mse, Tensor)
        # GAN loss in cLR-GAN
        random_z = Variable(Tensor(np.random.normal(0, 1, (batch_size,latent_dim))), requires_grad=False)
        fake_B_cLR = generator(real_A, random_z)
        loss_G_cLR = loss_discriminator_fix_D(fake_B_cLR, D_LR, criterion_mse, Tensor)
        loss_G_gan_vae = loss_G_cVAE + loss_G_cLR
        # KL-divergence loss in cVAE-GAN
        KL_div = lambda_kl * kld(mu, logvar)
        # Reconstruction loss in cVAE-GAN
        loss_l1_image = lambda_pixel * loss_generator(generator, real_A, z, real_B, criterion_l1)
        loss_EG = loss_G_gan_vae + KL_div + loss_l1_image
        all_zero_grad(optimizer_G, optimizer_E, optimizer_D_VAE, optimizer_D_LR)
        loss_EG.backward(retain_graph=True)
        optimizer_G.step()
        optimizer_E.step()
        #-------------------------------
        #  Train only Generator
        #------------------------------

        loss_l1_z = z_loss(fake_B_cLR, encoder, random_z, criterion_l1)
        loss_G = lambda_latent * loss_l1_z
        all_zero_grad(optimizer_G, optimizer_E, optimizer_D_VAE, optimizer_D_LR)
        loss_G.backward()
        optimizer_G.step()

        """ Optional TODO: 
            1. You may want to visualize results during training for debugging purpose
            2. Save your model every few iterations
        """
    break







